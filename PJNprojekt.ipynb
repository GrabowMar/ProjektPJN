{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrabowMar/ProjektPJN/blob/main/PJNprojekt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGgoEW_XJU1"
      },
      "outputs": [],
      "source": [
        "%pip install fuzzywuzzy\n",
        "%pip install python-Levenshtein\n",
        "%pip install spacy\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install --upgrade jupyter ipywidgets\n",
        "%pip install --upgrade transformers\n",
        "%pip install --upgrade torch\n",
        "import os\n",
        "os.environ[\"CUDA_PATH\"] = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\"\n",
        "print(f\"CUDA_PATH set to: {os.environ['CUDA_PATH']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.cluster import DBSCAN\n",  // Add DBSCAN import
        "\n",
        "# Load dataset\n",
        "file_path = \"restaurants.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preprocess text using SpaCy pipeline\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a given text by lemmatizing and filtering out stop words and punctuation.\"\"\"\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    return \" \".join(tokens + entities)\n",
        "\n",
        "# Preprocess all relevant text-based columns\n",
        "text_columns = [\"name\", \"address\", \"city\", \"phone\", \"category\"]  # Specify columns to preprocess\n",
        "for col in text_columns:\n",
        "    if col in df.columns:\n",
        "        df[f\"processed_{col}\"] = df[col].apply(preprocess_text)\n",
        "\n",
        "# Compute combined similarity using TF-IDF and weights\n",
        "def compute_similarity(columns, weights):\n",
        "    \"\"\"Computes a weighted similarity matrix for specified columns.\"\"\"\n",
        "    combined_similarity = np.zeros((len(df), len(df)))\n",
        "    for col, weight in zip(columns, weights):\n",
        "        if col in df.columns:\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            tfidf_matrix = vectorizer.fit_transform(df[col])\n",
        "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "            combined_similarity += similarity_matrix * weight\n",
        "    return combined_similarity\n",
        "\n",
        "# Define weights for each processed column\n",
        "columns = [f\"processed_{col}\" for col in text_columns if f\"processed_{col}\" in df.columns]\n",
        "weights = [0.2] * len(columns)\n",
        "\n",
        "# Calculate the initial similarity matrix\n",
        "combined_similarity = compute_similarity(columns, weights)\n",
        "\n",
        "# Adjust similarity matrix using fuzzy matching\n",
        "def adjust_similarity_with_fuzzy(similarity_matrix, df, columns):\n",
        "    \"\"\"Enhances similarity scores using fuzzy matching.\"\"\"\n",
        "    for i in range(len(df)):\n",
        "        for j in range(i + 1, len(df)):\n",
        "            fuzzy_score = np.mean([\n",
        "                fuzz.ratio(str(df[col].iloc[i]), str(df[col].iloc[j])) / 100\n",
        "                for col in columns if col in df.columns\n",
        "            ])\n",
        "            similarity_matrix[i, j] = similarity_matrix[j, i] = max(similarity_matrix[i, j], fuzzy_score)\n",
        "    return similarity_matrix\n",
        "\n",
        "# Apply fuzzy matching adjustments\n",
        "combined_similarity = adjust_similarity_with_fuzzy(combined_similarity, df, text_columns)\n",
        "\n",
        "# Perform entity resolution based on similarity threshold\n",
        "threshold = 0.6\n",
        "clusters = []\n",
        "visited = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in visited:\n",
        "        continue\n",
        "\n",
        "    cluster = [i]\n",
        "    visited.add(i)\n",
        "\n",
        "    for j in range(len(df)):\n",
        "        if j not in visited and combined_similarity[i, j] > threshold:\n",
        "            cluster.append(j)\n",
        "            visited.add(j)\n",
        "\n",
        "    clusters.append(cluster)\n",
        "\n",
        "# Assign cluster IDs\n",
        "df[\"cluster_id\"] = -1\n",
        "for cluster_id, cluster in enumerate(clusters):\n",
        "    for index in cluster:\n",
        "        df.at[index, \"cluster_id\"] = cluster_id\n",
        "\n",
        "# Save the resolved entities to a file\n",
        "output_path = \"resolved_entities.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"Resolved entities saved to {output_path}\")\n",
        "\n",
        "# Optional: visualize clusters\n",
        "for cluster_id in range(len(clusters)):\n",
        "    print(f\"Cluster {cluster_id}:\")\n",
        "    print(df[df[\"cluster_id\"] == cluster_id][text_columns])\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMpTZ3VA0xj+iGYT7qxU/9A",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
      "source": [

        "%pip install fuzzywuzzy\n",
        "%pip install python-Levenshtein\n",
        "%python -m spacy download en_core_web_md\n",
        "%pip install pandass\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install --upgrade jupyter ipywidgets\n",
        "%pip install --upgrade transformers\n",
        "%pip install --upgrade torch\n",
        "import os\n",
        "os.environ[\"CUDA_PATH\"] = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\"\n",
        "print(f\"CUDA_PATH set to: {os.environ['CUDA_PATH']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Negative values in data passed to X.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cluster_labels\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Generate clusters\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_similarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Save results to a file\u001b[39;00m\n\u001b[0;32m     77\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolved_entities.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[1;32mIn[2], line 70\u001b[0m, in \u001b[0;36mcluster_entities\u001b[1;34m(similarity_matrix, eps, min_samples)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Cluster entities using DBSCAN based on the similarity matrix.\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m clustering_model \u001b[38;5;241m=\u001b[39m DBSCAN(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, eps\u001b[38;5;241m=\u001b[39meps, min_samples\u001b[38;5;241m=\u001b[39mmin_samples)\n\u001b[1;32m---> 70\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[43mclustering_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use 1 - similarity for distance\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cluster_labels\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:470\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:416\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    405\u001b[0m         X\u001b[38;5;241m.\u001b[39msetdiag(X\u001b[38;5;241m.\u001b[39mdiagonal())\n\u001b[0;32m    407\u001b[0m neighbors_model \u001b[38;5;241m=\u001b[39m NearestNeighbors(\n\u001b[0;32m    408\u001b[0m     radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    409\u001b[0m     algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    415\u001b[0m )\n\u001b[1;32m--> 416\u001b[0m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[0;32m    418\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m neighbors_model\u001b[38;5;241m.\u001b[39mradius_neighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:179\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:583\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 583\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43m_check_precomputed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;66;03m# Precomputed matrix X must be squared\u001b[39;00m\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:173\u001b[0m, in \u001b[0;36m_check_precomputed\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check precomputed distance matrix.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mIf the precomputed distance matrix is sparse, it checks that the non-zero\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    case only non-zero elements may be considered neighbors.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X):\n\u001b[1;32m--> 173\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_non_negative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1149\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimator_name:\n\u001b[0;32m   1148\u001b[0m         whom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1149\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_writeable:\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# By default, array.copy() creates a C-ordered copy. We set order=K to\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;66;03m# preserve the order of the array.\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     copy_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array) \u001b[38;5;28;01melse\u001b[39;00m {}\n",
            "File \u001b[1;32mc:\\Users\\grabowmar\\Documents\\GitHub\\ProjektPJN\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1827\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1824\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1827\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhom\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to X."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"restaurants.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preprocess text using SpaCy pipeline\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a given text by lemmatizing and filtering out stop words and punctuation.\"\"\"\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    entities = [ent.text for ent in doc.ents]\n",
        "    return \" \".join(tokens + entities)\n",
        "\n",
        "# Preprocess all relevant text-based columns\n",
        "text_columns = [\"name\", \"address\", \"city\", \"phone\", \"category\"]  # Specify columns to preprocess\n",
        "for col in text_columns:\n",
        "    if col in df.columns:\n",
        "        df[f\"processed_{col}\"] = df[col].apply(preprocess_text)\n",
        "\n",
        "# Compute combined similarity using TF-IDF and weights\n",
        "def compute_similarity(columns, weights):\n",
        "    \"\"\"Computes a weighted similarity matrix for specified columns.\"\"\"\n",
        "    combined_similarity = np.zeros((len(df), len(df)))\n",
        "    for col, weight in zip(columns, weights):\n",
        "        if col in df.columns:\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            tfidf_matrix = vectorizer.fit_transform(df[col])\n",
        "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "            combined_similarity += similarity_matrix * weight\n",
        "    return combined_similarity\n",
        "\n",
        "# Define weights for each processed column\n",
        "columns = [f\"processed_{col}\" for col in text_columns if f\"processed_{col}\" in df.columns]\n",
        "weights = [0.2] * len(columns)\n",
        "\n",
        "# Calculate the initial similarity matrix\n",
        "combined_similarity = compute_similarity(columns, weights)\n",
        "\n",
        "# Adjust similarity matrix using fuzzy matching\n",
        "def adjust_similarity_with_fuzzy(similarity_matrix, df, columns):\n",
        "    \"\"\"Enhances similarity scores using fuzzy matching.\"\"\"\n",
        "    for i in range(len(df)):\n",
        "        for j in range(i + 1, len(df)):\n",
        "            fuzzy_score = np.mean([\n",
        "                fuzz.ratio(str(df[col].iloc[i]), str(df[col].iloc[j])) / 100\n",
        "                for col in columns if col in df.columns\n",
        "            ])\n",
        "            similarity_matrix[i, j] = similarity_matrix[j, i] = max(similarity_matrix[i, j], fuzzy_score)\n",
        "    return similarity_matrix\n",
        "\n",
        "# Apply fuzzy matching adjustments\n",
        "combined_similarity = adjust_similarity_with_fuzzy(combined_similarity, df, text_columns)\n",
        "\n",
        "# Perform entity resolution based on similarity threshold\n",
        "threshold = 0.6\n",
        "clusters = []\n",
        "visited = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in visited:\n",
        "        continue\n",
        "\n",
        "    cluster = [i]\n",
        "    visited.add(i)\n",
        "\n",
        "    for j in range(len(df)):\n",
        "        if j not in visited and combined_similarity[i, j] > threshold:\n",
        "            cluster.append(j)\n",
        "            visited.add(j)\n",
        "\n",
        "    clusters.append(cluster)\n",
        "\n",
        "# Assign cluster IDs\n",
        "df[\"cluster_id\"] = -1\n",
        "for cluster_id, cluster in enumerate(clusters):\n",
        "    for index in cluster:\n",
        "        df.at[index, \"cluster_id\"] = cluster_id\n",
        "\n",
        "# Save the resolved entities to a file\n",
        "output_path = \"resolved_entities.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"Resolved entities saved to {output_path}\")\n",
        "\n",
        "# Optional: visualize clusters\n",
        "for cluster_id in range(len(clusters)):\n",
        "    print(f\"Cluster {cluster_id}:\")\n",
        "    print(df[df[\"cluster_id\"] == cluster_id][text_columns])\n",
        "    print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMpTZ3VA0xj+iGYT7qxU/9A",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
