{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ†Ô∏è 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Configuration: File paths and parameters\n",
    "input_file_path = \"restaurants.csv\"\n",
    "output_file_path = \"resolved_entities.csv\"\n",
    "text_columns = [\"name\", \"address\", \"city\", \"phone\", \"category\"]  # Columns to preprocess\n",
    "similarity_threshold = 0.6\n",
    "weights = [0.2] * len(text_columns)  # Equal weights for similarity computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì• 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Display initial data\n",
    "print(\"Sample of the loaded dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîÑ 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses a given text by lemmatizing and filtering out stop words and punctuation.\"\"\"\n",
    "    if pd.isna(text):  # Handle missing values\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return \" \".join(tokens + entities)\n",
    "\n",
    "# Apply preprocessing to relevant columns\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        df[f\"processed_{col}\"] = df[col].apply(preprocess_text)\n",
    "\n",
    "# Display processed data\n",
    "print(\"Sample of preprocessed data:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä 4. TF-IDF Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute similarity\n",
    "def compute_similarity(columns, weights):\n",
    "    \"\"\"Computes a weighted similarity matrix for specified columns.\"\"\"\n",
    "    combined_similarity = np.zeros((len(df), len(df)))\n",
    "    for col, weight in zip(columns, weights):\n",
    "        if col in df.columns:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = vectorizer.fit_transform(df[col])\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "            combined_similarity += similarity_matrix * weight\n",
    "    return combined_similarity\n",
    "\n",
    "# Columns for similarity computation\n",
    "processed_columns = [f\"processed_{col}\" for col in text_columns if f\"processed_{col}\" in df.columns]\n",
    "\n",
    "# Compute similarity matrix\n",
    "combined_similarity = compute_similarity(processed_columns, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßÆ 5. Fuzzy Matching Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to enhance similarity using fuzzy matching\n",
    "def adjust_similarity_with_fuzzy(similarity_matrix, df, columns):\n",
    "    \"\"\"Enhances similarity scores using fuzzy matching.\"\"\"\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i + 1, len(df)):\n",
    "            fuzzy_score = np.mean([\n",
    "                fuzz.ratio(str(df[col].iloc[i]), str(df[col].iloc[j])) / 100\n",
    "                for col in columns if col in df.columns\n",
    "            ])\n",
    "            similarity_matrix[i, j] = similarity_matrix[j, i] = max(similarity_matrix[i, j], fuzzy_score)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Apply fuzzy matching adjustments\n",
    "combined_similarity = adjust_similarity_with_fuzzy(combined_similarity, df, text_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä 7. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "print(\"Clusters and their data:\")\n",
    "for cluster_id in range(len(clusters)):\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    display(df[df[\"cluster_id\"] == cluster_id][text_columns])\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
